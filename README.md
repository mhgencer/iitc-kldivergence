By MH Gencer

Go to 'code.py' above for the Python script (requires Python 3.9 or higher).


Click [here](https://iitc-kldivergence.github.io/mathlet/) for a browser-based demo. See below for a short explanation. 


![Figure_1](https://user-images.githubusercontent.com/97817068/210168473-e4d863bc-e177-41c4-85d8-c0cdb4a49a91.png)


Left: Python script output (matplotlib). Right: Browser-based demo (Chart.js). The following applies to both.

## Integrated Information Theory of Consciousness (IITC)
IITC is a relatively new theory of consciousness proposed by Giulio Tononi (2008, 2014, 2016). The theory's main goal is to establish the foundations for a mathematical framework with which we can answer the question of why we are conscious. In the framework IITC offers, consciousness results from highly integrated informational processes that can be specified by using a set of well-defined mathematical concepts. The hope is to open new avenues for both empirical and purely mathematical research on necessary and sufficient conditions for having conscious experience.


## Kullback-Leibler Divergence (KLD)
KLD is a measure of the difference between two probability distributions. In the IITC framework, KLD is used to conceptualize the difference between what the conscious entity generates as output (i.e., conscious experience) and what its physical substrate of consciousness (e.g., brain) takes as input (e.g., sense-data), both construed as probability distributions. More specifically, from an information-theoretic perspective, KLD calculates how many bits of information (nats) are lost in the process where the probabilistic input a system receives turns into the system's actual output (in a way contingent upon the randomly actualized value of the input). For related reasons, KLD has been argued to be the best measure of 'integrated information', the key concept used in the definition of consciousness in IITC (Amari 2016, Griffith 2014).


In the interactive plot generated by 'code.py', KLD is written in the standard notation KL(P || Q), which returns the value of the statistical distance from the distribution P to the distribution Q, measured in terms of nats. Users can toggle between possible μ (mean) and σ (standard deviation) values for "Blue" (P) and "Red" (Q) normal probability distributions, and see how KLD value changes as the distributions become more desynchronized. In the browser-based demo, (P) and (Q) are referred to as "Dark" and "Fair", respectively.


## References
Amari, S. I. (2016). Information Geometry and Its Applications (Vol. 194). Springer.


Griffith, V. (2014). A principled infotheoretic\phi-like measure. arXiv preprint arXiv:1401.0978.


Oizumi, M., Albantakis, L., & Tononi, G. (2014). From the phenomenology to the mechanisms of consciousness: integrated information theory 3.0. PLoS computational biology, 10(5), e1003588.


Tononi, G. (2008). Consciousness as integrated information: a provisional manifesto. The Biological Bulletin, 215(3), 216-242.


Tononi, G., Boly, M., Massimini, M., & Koch, C. (2016). Integrated information theory: from consciousness to its physical substrate. Nature Reviews Neuroscience, 17(7), 450-461.

